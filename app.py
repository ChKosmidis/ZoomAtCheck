import streamlit as st
import pandas as pd
import re
import difflib

def clean_text(text):
    if not isinstance(text, str):
        return ""
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    return re.sub(r'\W+', ' ', text).lower().strip()

def get_tokens(text):
    return set(clean_text(text).split())

def strict_clean(text):
    # –î–ª—è fuzzy matching —É–±–∏—Ä–∞–µ–º –≤–æ–æ–±—â–µ –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
    return re.sub(r'\W+', '', str(text)).lower()

def process_files(df_signup, df_attendance):
    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤ —Ñ–∞–π–ª–µ "–∑–∞–ø–∏—Å–∞–ª–∏—Å—å" –∏–º–µ–Ω–∞ –≤ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ
    signup_col = df_signup.columns[0]
    # –í —Ñ–∞–π–ª–µ "–ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å" –∏—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–ª–∏ –±–µ—Ä–µ–º 0 –∏ 1
    att_name_col = df_attendance.columns[0] # –û–±—ã—á–Ω–æ "–ò–º—è (–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –∏–º—è)"
    att_dur_col = df_attendance.columns[1]  # –û–±—ã—á–Ω–æ "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–∏–Ω)"

    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
    for col in df_attendance.columns:
        if "–ò–º—è" in col: att_name_col = col
        if "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å" in col: att_dur_col = col

    signup_names = df_signup[signup_col].dropna().unique()
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–∞—Ç—á–∏–Ω–≥–∞
    signup_specs = []
    for name in signup_names:
        c_name = clean_text(name)
        tokens = get_tokens(name)
        signup_specs.append({
            'original': name, 
            'clean': c_name, 
            'tokens': tokens,
            'strict': strict_clean(name)
        })

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏
    df_attendance['normalized_name'] = None

    # --- –≠–¢–ê–ü 1: –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –ø–æ–∏—Å–∫ –ø–æ —Ç–æ–∫–µ–Ω–∞–º ---
    for idx, row in df_attendance.iterrows():
        att_name_raw = row[att_name_col]
        att_tokens = get_tokens(att_name_raw)
        
        best_match = None
        best_score = 0
        
        for spec in signup_specs:
            s_tokens = spec['tokens']
            if not s_tokens or not att_tokens:
                continue
            
            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
            common = s_tokens.intersection(att_tokens)
            score = len(common)
            
            # –õ–æ–≥–∏–∫–∞: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ 2 –∏ –±–æ–ª–µ–µ —Å–ª–æ–≤ - —ç—Ç–æ –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –º–∞—Ç—á
            # –ò–ª–∏ –µ—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ, –Ω–æ –æ–Ω–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å—ë –∏–º—è (—Ä–µ–¥–∫–∏–µ —Å–ª—É—á–∞–∏)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ (–µ—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞—è–≤–∫–∏ –µ—Å—Ç—å –≤ –ø–æ—Å–µ—â–µ–Ω–∏–∏ –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç)
            is_subset = s_tokens.issubset(att_tokens) or att_tokens.issubset(s_tokens)
            
            if score >= 2: # –°–∏–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if score > best_score:
                    best_score = score
                    best_match = spec['original']
            elif score == 1 and is_subset and (len(s_tokens) == 1 or len(att_tokens) == 1):
                 # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ 1 —Å–ª–æ–≤—É, –µ—Å–ª–∏ —Å–∞–º–æ –∏–º—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 1 —Å–ª–æ–≤–∞
                 if score > best_score:
                    best_score = score
                    best_match = spec['original']

        if best_match:
            df_attendance.at[idx, 'normalized_name'] = best_match

    # --- –≠–¢–ê–ü 2: Fuzzy Matching (–¥–ª—è —Ç–µ—Ö, –∫—Ç–æ –Ω–µ –Ω–∞—à–µ–ª—Å—è) ---
    # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ—Ö, –∫–æ–≥–æ –º—ã –ï–©–ï –ù–ï –Ω–∞—à–ª–∏ –≤ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏ (–∏–∑ —Å–ø–∏—Å–∫–∞ –∑–∞–ø–∏—Å–∞–≤—à–∏—Ö—Å—è)
    found_names = set(df_attendance['normalized_name'].dropna().unique())
    missing_signup_names = [n for n in signup_names if n not in found_names]
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞: {–æ—á–∏—â–µ–Ω–Ω–æ–µ_–∏–º—è : –æ—Ä–∏–≥–∏–Ω–∞–ª}
    missing_signup_map = {strict_clean(name): name for name in missing_signup_names}
    missing_keys = list(missing_signup_map.keys())

    unmatched_indices = df_attendance[df_attendance['normalized_name'].isnull()].index
    
    for idx in unmatched_indices:
        att_raw = df_attendance.at[idx, att_name_col]
        att_clean = strict_clean(att_raw)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–µ–µ —Å—Ä–µ–¥–∏ –ø–æ—Ç–µ—Ä—è—à–µ–∫
        # cutoff=0.6 - –ø–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —Ä–µ—à–µ–Ω–∏–∏)
        matches = difflib.get_close_matches(att_clean, missing_keys, n=1, cutoff=0.6)
        
        if matches:
            matched_key = matches[0]
            original_name = missing_signup_map[matched_key]
            df_attendance.at[idx, 'normalized_name'] = original_name

    # --- –†–ê–°–ß–ï–¢ –°–¢–ê–¢–ò–°–¢–ò–ö–ò ---
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∏–º–µ–Ω–∏
    duration_stats = df_attendance.groupby('normalized_name')[att_dur_col].sum().reset_index()
    
    present_names = set(duration_stats['normalized_name'])
    all_signup = set(signup_names)
    
    # –°–ø–∏—Å–æ–∫ 1: –ó–∞–ø–∏—Å–∞–ª–∏—Å—å, –Ω–æ –Ω–µ –±—ã–ª–∏
    not_present = sorted(list(all_signup - present_names))
    
    # –°–ø–∏—Å–æ–∫ 2: –ë—ã–ª–∏ –º–µ–Ω—å—à–µ 90 –º–∏–Ω—É—Ç
    under_90 = duration_stats[duration_stats[att_dur_col] < 90].sort_values('normalized_name')
    under_90_list = under_90[['normalized_name', att_dur_col]].values.tolist() # [[Name, Time], ...]

    return not_present, under_90_list, df_attendance

# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏", layout="wide")

st.title("üìä –°–≤–µ—Ä–∫–∞ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏ –≤–µ–±–∏–Ω–∞—Ä–∞")
st.markdown("""
–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–≤–∞ —Ñ–∞–π–ª–∞:
1. **–°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–∞–≤—à–∏—Ö—Å—è** (–æ–±—ã—á–Ω–æ –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å –∏–º–µ–Ω–∞–º–∏).
2. **–û—Ç—á–µ—Ç –æ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏** (CSV –∏–∑ Zoom/Webinar, —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ "–ò–º—è" –∏ "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å").
""")

col1, col2 = st.columns(2)

with col1:
    file_signup = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª '–ó–∞–ø–∏—Å–∞–ª–∏—Å—å' (.csv)", type=['csv'])

with col2:
    file_attendance = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª '–ü–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å' (.csv)", type=['csv'])

if file_signup and file_attendance:
    st.divider()
    
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, —Ç–∞–∫ –∫–∞–∫ –≤ –ø–µ—Ä–≤–æ–º —Ñ–∞–π–ª–µ –∑–∞–ø—è—Ç–∞—è, –≤–æ –≤—Ç–æ—Ä–æ–º —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π
        try:
            df_s = pd.read_csv(file_signup)
            if df_s.shape[1] < 1: # –ï—Å–ª–∏ –Ω–µ —Å—á–∏—Ç–∞–ª–æ—Å—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                 df_s = pd.read_csv(file_signup, sep=';')
        except:
             st.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∑–∞–ø–∏—Å–∏.")
        
        try:
            df_a = pd.read_csv(file_attendance, sep=';') # –ß–∞—â–µ –≤—Å–µ–≥–æ –æ—Ç—á–µ—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ ;
            if df_a.shape[1] < 2:
                df_a = pd.read_csv(file_attendance, sep=',')
        except:
             st.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏.")

        st.info("–§–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –û–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        # –ó–∞–ø—É—Å–∫ –ª–æ–≥–∏–∫–∏
        not_present, under_90, df_debug = process_files(df_s, df_a)
        
        # --- –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
        
        st.success("–ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∏–∂–µ.")
        
        res_col1, res_col2 = st.columns(2)
        
        with res_col1:
            st.subheader(f"üî¥ –ó–∞–ø–∏—Å–∞–ª–∏—Å—å, –Ω–æ –Ω–µ –±—ã–ª–∏ ({len(not_present)})")
            st.write("–≠—Ç–∏ –ª—é–¥–∏ –µ—Å—Ç—å –≤ –ø–µ—Ä–≤–æ–º —Ñ–∞–π–ª–µ, –Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤–æ –≤—Ç–æ—Ä–æ–º.")
            
            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            df_not_present = pd.DataFrame(not_present, columns=["–ò–º—è"])
            st.dataframe(df_not_present, height=400, use_container_width=True)
            
            # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            csv_not = df_not_present.to_csv(index=False).encode('utf-8')
            st.download_button(
                "–°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö",
                data=csv_not,
                file_name="–Ω–µ_–ø—Ä–∏—à–ª–∏.csv",
                mime="text/csv"
            )

        with res_col2:
            st.subheader(f"üü° –ë—ã–ª–∏ –º–µ–Ω–µ–µ 90 –º–∏–Ω—É—Ç ({len(under_90)})")
            st.write("–°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è —É—á–∞—Å—Ç–∏—è –º–µ–Ω—å—à–µ 1.5 —á–∞—Å–æ–≤.")
            
            df_under_90 = pd.DataFrame(under_90, columns=["–ò–º—è", "–í—Ä–µ–º—è (–º–∏–Ω)"])
            st.dataframe(df_under_90, height=400, use_container_width=True)
            
            csv_under = df_under_90.to_csv(index=False).encode('utf-8')
            st.download_button(
                "–°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫ (<90 –º–∏–Ω)",
                data=csv_under,
                file_name="–º–µ–Ω–µ–µ_90_–º–∏–Ω—É—Ç.csv",
                mime="text/csv"
            )

        with st.expander("üîé –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)"):
            st.write("–ù–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–ª–∞ –∏–º–µ–Ω–∞ –∏–∑ –æ—Ç—á–µ—Ç–∞ —Å –∏–º–µ–Ω–∞–º–∏ –∏–∑ –∑–∞–ø–∏—Å–∏.")
            st.dataframe(df_debug[['–ò–º—è (–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –∏–º—è)', 'normalized_name', '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–∏–Ω)']].dropna())

    except Exception as e:
        st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        st.write("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç CSV —Ñ–∞–π–ª–æ–≤ (–∫–æ–¥–∏—Ä–æ–≤–∫—É –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏).")