import streamlit as st
import pandas as pd
import re
import difflib

# --- 1. –õ–û–ì–ò–ö–ê –û–ß–ò–°–¢–ö–ò (–° –§–ò–õ–¨–¢–†–û–ú –®–ï–ë) ---

def clean_text(text):
    if not isinstance(text, str): return ""
    # –û—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
    return re.sub(r'[^\w\s]', ' ', text).lower().strip()

def get_tokens(text):
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    cleaned = clean_text(text)
    words = cleaned.split()
    
    # !!! –°–ü–ò–°–û–ö –°–¢–û–ü-–°–õ–û–í !!!
    # –≠—Ç–∏ —Å–ª–æ–≤–∞ –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏
    stop_words = {
        '—à–µ–±', 'sheb', '—à–∫–æ–ª–∞', '–ø—Ä–∞–≤', '—á–µ–ª–æ–≤–µ–∫–∞', 
        '–∏–º–µ–Ω–∏', '–µ–ª–µ–Ω—ã', '–±–æ–Ω–Ω—ç—Ä', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '–≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤',
        'she', 'b' # –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ä–∞–∑–æ–±—å–µ—Ç—Å—è
    }
    
    valid_tokens = set()
    for w in words:
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –º—É—Å–æ—Ä (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if w not in stop_words and len(w) > 1:
            valid_tokens.add(w)
            
    return valid_tokens

def strict_clean(text):
    # –î–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —É–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –ø—Ä—è–º–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏
    text = text.lower()
    noise = ['—à–µ–±', 'sheb', '—à–∫–æ–ª–∞ –ø—Ä–∞–≤ —á–µ–ª–æ–≤–µ–∫–∞', '|']
    for n in noise:
        text = text.replace(n, '')
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã
    return re.sub(r'\W+', '', text)

# --- 2. –õ–û–ì–ò–ö–ê –°–í–ï–†–ö–ò ---

def process_files(df_signup, df_attendance, threshold_minutes):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
    signup_col = df_signup.columns[0]
    att_name_col = df_attendance.columns[0]
    att_dur_col = df_attendance.columns[1]

    for col in df_attendance.columns:
        if "–ò–º—è" in col: att_name_col = col
        if "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å" in col: att_dur_col = col

    signup_names = df_signup[signup_col].dropna().unique()
    
    # –ì–æ—Ç–æ–≤–∏–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–∞–≤—à–∏—Ö—Å—è
    signup_specs = []
    for name in signup_names:
        signup_specs.append({
            'original': name, 
            'tokens': get_tokens(name), # <-- –¢—É—Ç –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä
        })

    df_attendance['normalized_name'] = None

    # --- –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º ---
    for idx, row in df_attendance.iterrows():
        att_name_raw = row[att_name_col]
        att_tokens = get_tokens(att_name_raw) # <-- –ò —Ç—É—Ç —Ç–æ–∂–µ
        
        best_match = None
        best_score = 0
        
        for spec in signup_specs:
            s_tokens = spec['tokens']
            if not s_tokens or not att_tokens: continue
            
            common = s_tokens.intersection(att_tokens)
            score = len(common)
            
            # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ 2+ —Å–ª–æ–≤ –∏–ª–∏ 1 —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            is_subset = s_tokens.issubset(att_tokens) or att_tokens.issubset(s_tokens)
            
            if score >= 2:
                if score > best_score:
                    best_score = score
                    best_match = spec['original']
            elif score == 1 and is_subset:
                 if score > best_score:
                    best_score = score
                    best_match = spec['original']

        if best_match:
            df_attendance.at[idx, 'normalized_name'] = best_match

    # --- –≠–¢–ê–ü 2: Fuzzy Search (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫) ---
    found_names = set(df_attendance['normalized_name'].dropna().unique())
    missing_signup_names = [n for n in signup_names if n not in found_names]
    
    missing_map = {strict_clean(name): name for name in missing_signup_names}
    missing_keys = list(missing_map.keys())

    unmatched_indices = df_attendance[df_attendance['normalized_name'].isnull()].index
    
    for idx in unmatched_indices:
        att_raw = df_attendance.at[idx, att_name_col]
        att_clean = strict_clean(att_raw) # <-- –£–±–∏—Ä–∞–µ–º –®–ï–ë –ø–µ—Ä–µ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º
        
        matches = difflib.get_close_matches(att_clean, missing_keys, n=1, cutoff=0.6)
        
        if matches:
            matched_key = matches[0]
            df_attendance.at[idx, 'normalized_name'] = missing_map[matched_key]

    # --- –°–¢–ê–¢–ò–°–¢–ò–ö–ê ---
    # –°—É–º–º–∏—Ä—É–µ–º –≤—Ä–µ–º—è
    stats = df_attendance.groupby('normalized_name')[att_dur_col].sum().reset_index()
    stats.columns = ['–ò–º—è —É—á–∞—Å—Ç–Ω–∏–∫–∞', '–í—Ä–µ–º—è (–º–∏–Ω)']
    stats = stats.sort_values(by='–í—Ä–µ–º—è (–º–∏–Ω)', ascending=False)
    
    present_names = set(stats['–ò–º—è —É—á–∞—Å—Ç–Ω–∏–∫–∞'])
    all_signup = set(signup_names)
    
    # 1. –ù–µ –ø—Ä–∏—à–ª–∏
    not_present = sorted(list(all_signup - present_names))
    
    # 2. –ú–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏
    under_threshold = stats[stats['–í—Ä–µ–º—è (–º–∏–Ω)'] < threshold_minutes]
    
    return not_present, under_threshold, df_attendance, stats

# --- 3. –ò–ù–¢–ï–†–§–ï–ô–° ---

st.set_page_config(page_title="–°–≤–µ—Ä–∫–∞ Zoom", layout="wide")
st.title("üìä –°–≤–µ—Ä–∫–∞ Zoom (—Å –∏–≥–Ω–æ—Ä–æ–º '–®–ï–ë')")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
with st.container():
    col1, col2 = st.columns([1, 2])
    with col1:
        threshold = st.number_input("–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–º–∏–Ω):", min_value=1, value=60, step=15)
    with col2:
        st.info(f"–°–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–ø–∏—Å–∫–∏: **–®–ï–ë, Sheb, —à–∫–æ–ª–∞ –ø—Ä–∞–≤ —á–µ–ª–æ–≤–µ–∫–∞** –∏ —Å–∏–º–≤–æ–ª—ã **| -**")

st.divider()

c1, c2 = st.columns(2)
with c1:
    f_signup = st.file_uploader("1. –§–∞–π–ª '–ó–∞–ø–∏—Å–∞–ª–∏—Å—å' (.csv)", type='csv')
with c2:
    f_att = st.file_uploader("2. –§–∞–π–ª '–ü–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å' (.csv)", type='csv')

if f_signup and f_att:
    st.divider()
    try:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã
        try:
            df_s = pd.read_csv(f_signup)
            if df_s.shape[1] < 1: df_s = pd.read_csv(f_signup, sep=';')
        except:
            st.error("–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ –∑–∞–ø–∏—Å–∏")
            st.stop()
            
        try:
            df_a = pd.read_csv(f_att, sep=';')
            if df_a.shape[1] < 2: df_a = pd.read_csv(f_att, sep=',')
        except:
            st.error("–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏")
            st.stop()

        # –ó–∞–ø—É—Å–∫
        not_present, under_limit, df_debug, df_full = process_files(df_s, df_a, threshold)
        
        st.success("–ì–æ—Ç–æ–≤–æ!")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        rc1, rc2 = st.columns(2)
        
        with rc1:
            st.subheader(f"üî¥ –ù–µ –ø—Ä–∏—à–ª–∏ ({len(not_present)})")
            df_not = pd.DataFrame(not_present, columns=["–ò–º—è"])
            st.dataframe(df_not, use_container_width=True, height=300)
            st.download_button("–°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫", df_not.to_csv(index=False).encode('utf-8'), "–Ω–µ_–ø—Ä–∏—à–ª–∏.csv", "text/csv")

        with rc2:
            st.subheader(f"üü° –ú–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏ (<{threshold} –º–∏–Ω) ({len(under_limit)})")
            st.dataframe(under_limit, use_container_width=True, height=300)
            st.download_button("–°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫", under_limit.to_csv(index=False).encode('utf-8'), "–º–∞–ª–æ_–≤—Ä–µ–º–µ–Ω–∏.csv", "text/csv")
            
        st.divider()
        st.subheader("üìã –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.dataframe(df_full, use_container_width=True)
        st.download_button("–°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—É—é —Ç–∞–±–ª–∏—Ü—É", df_full.to_csv(index=False).encode('utf-8'), "–ø–æ–ª–Ω–∞—è_—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.csv", "text/csv")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")